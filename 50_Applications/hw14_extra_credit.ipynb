{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aware-alexander",
   "metadata": {},
   "source": [
    "# Homework 14 (Extra Credit)\n",
    "\n",
    "In this assignment, we'll work through a few problems related to regression and least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-cartoon",
   "metadata": {},
   "source": [
    "## Problem 1: Fitting polynomials and ridge regression\n",
    "\n",
    "In this problem, we will practice fitting poynomials using least squares, and introduce ridge regression as a technique to limit the \"complexity\" of our fitted models.\n",
    "\n",
    "The polynomials we will fit are of the form:\n",
    "\n",
    "$$\n",
    "f(x) = b_0 + b_1x + b_2 x^2 + \\cdots + b_p x^p.\n",
    "$$\n",
    "\n",
    "Given $n$ training samples $(x_1,y_1),\\dots,(x_n,y_n)$, where $x_i \\in \\mathbb{R}$ and $y_i \\in \\mathbb{R}$, we define the matrix\n",
    "\n",
    "$$\n",
    "X_p = \\begin{pmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^p\\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^p\\\\ \\vdots & \\vdots & \\vdots & \\vdots &\\vdots \\\\ 1 & x_n & x_n^2 & \\cdots & x_n^p\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "which is an $n\\times (p+1)$ matrix. The given $y = (y_1,\\dots, y_n)$ and $b= (b_0,\\dots,b_p)$, we can fit the polynomials using the least squares criteria:\n",
    "\n",
    "$$\n",
    "\\hat{b}_p = \\text{arg}\\min_b \\|X_pb - y\\|_2^2 \\implies \\hat{b} = (X_p^\\top X_p)^{-1}X_p^\\top y.\n",
    "$$\n",
    "\n",
    "For this problem, we will use the training and testing data sets defined below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "normal-blend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data(n):\n",
    "    x = np.random.uniform(low=-1, high=1, size=n)\n",
    "    y = .01 * x + 10 * x**3 - 5*x**5 + np.random.standard_normal(n)\n",
    "    return x, y\n",
    "\n",
    "np.random.seed(10)\n",
    "x_train, y_train = get_data(100)\n",
    "x_test, y_test = get_data(500)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.title('training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-torture",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Define a function `polynomial_features(x,p)` which takes in a vector of input data $x\\in \\mathbb{R}^n$ and returns the matrix $X_p$ of polynomial features of degree $p$. Then, define a function `fit_ls(Xp,y)` which takes in a matrix $X_p$ and a vector of responses $y$ and returns the least squares solution $\\hat{b}_p = (X_p^\\top X_p)^{-1}X_p^\\top y$. (You should be able to use some code from the least squares notebook for this part.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-airplane",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-craft",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "For each $p\\in \\{1,2,3,\\dots,20\\}$, perform the following:\n",
    "\n",
    "- compute $\\hat{b}_p$.\n",
    "- create a matrix $X_{p,\\text{test}}$ (obtained from `polynomial_features(x_test,p)`), and compute and store the test error $E_p = \\|X_{p,\\text{test}}\\hat{b}_p - y_\\text{test}\\|_2^2$.\n",
    "- plot the errors $E_p$ as a function of $p$. Which value of $p$ has the lowest test error? What happens as $p$ gets very large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-lotus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "listed-april",
   "metadata": {},
   "source": [
    "### Part C\n",
    "As we (hopefully) saw in Part B, as we fit a higher degree polynomial, we start to \"overfit\" the data, resulting in high test error. In this part, we show that one way to avoid this is by using _ridge regression_. In ridge regression, rather that using the using least squares objective, we pick coefficients $\\hat{b}_\\lambda$ which minimize the following objective:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{b}_\\lambda = \\text{arg}\\min_b \\|Xb - y\\|_2^2 + \\lambda \\|b\\|_2^2 && (1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The additional term $\\lambda \\|b\\|_2^2$ is a \"penalty\" term, which penalizes our model for having too large of coefficients. In practice, this penalty can help prevent us from overfitting the data like we did in Part B. Using some matrix calculus, it can be shown that the solution to (1) is given by\n",
    "\n",
    "$$\n",
    "\\hat{b}_\\lambda = (X^\\top X + \\lambda I)^{-1}X^\\top y.\n",
    "$$\n",
    "\n",
    "where $I$ is the $(p+1)\\times (p+1)$ identity matrix. For this problem, we will use the same training and testing data as before, and fix $p=20$. Perform the following:\n",
    "\n",
    "- for each $\\lambda$ in `np.arange(0,5,.1)`, compute $\\hat{b}_\\lambda = (X_{p}^\\top X_{p} + \\lambda I)^{-1}X_{p}^\\top y$\n",
    "- compute and store the test error $E_\\lambda = \\|X_{p,\\text{test}}\\hat{b}_\\lambda - y_{\\text{test}}\\|_2^2$\n",
    "- plot the test error $E_\\lambda$ as a function of $\\lambda$\n",
    "\n",
    "What value of $\\lambda$ gives the smallest test error? How does this error compare to the $p=20$ error from Part B? How does this error compare to the best model fit in Part B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-combining",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rural-crowd",
   "metadata": {},
   "source": [
    "## Problem 2: gradient descent for least squares\n",
    "\n",
    "In this problem, we investigate the _gradient algorithm_ for solving the least squares problem. Gradient descent algorithms are generic algorithms which can be used to minimize functions $f(x)$. Generically, the algorithm takes the form\n",
    "\n",
    "$$\n",
    "x^{(t+1)} = x^{(t)} - \\alpha\\cdot \\nabla f(x^{(t)})\n",
    "$$\n",
    "\n",
    "where $\\nabla f(x^{(t)})$ is the _gradient_ of $f$ (i.e. the vector whose $j$th entry is $\\partial f(x)/\\partial x_j$.)\n",
    "\n",
    "In the case of least squares, our objective function is $f(b) = \\|Xb - y\\|_2^2$, for which the gradient is\n",
    "\n",
    "$$\n",
    "\\nabla_b f(b) = 2(X^\\top X b - X^\\top y)\n",
    "$$\n",
    "\n",
    "which gives the gradient descent updates:\n",
    "\n",
    "$$\n",
    "b^{(t+1)} = b^{(t)} - 2\\alpha\\cdot  (X^\\top X b^{(t)} - X^\\top y).\n",
    "$$\n",
    "\n",
    "We can initialize $b^{(0)}$ to be, for example, a random vector. There are two more missing pieces that we need to run the gradient descent algorithm: 1) we need to choose the step size $\\alpha$ and 2) we need to have a criteria for when to stop the iterations. In the case of linear regression, it turns out that the gradient descent algorithm will work for any $0 < \\alpha < \\frac{1}{2\\lambda_{max}(X^\\top X)}$, where $\\lambda_{max}(X^\\top X)$ is the largest eigenvalue of $X^\\top X$. To decide how many iterations $T$ we need to run, a simple rule of thumb is to continue until $\\|\\nabla_b f(b^{(t)})\\|_2$ is sufficiently small, say below some threshold $\\epsilon$ (this means that $\\nabla_b f(b^{(t)}) \\approx 0$, so that we are approximately at the minimum).\n",
    "\n",
    "**Remark:** Iterative algorithms like gradient descent and Newton's method are rarely used in practice with linear least squares problems, since we have closed-form solutions that are usually used in practice. However, these algorithms are much more general, and can be applied to a wide variety of minimization problems that don't admit closed-form solutions. For example, gradient descent (and variants of it) are the standard method for training deep neural networks.\n",
    "\n",
    "For this problem, we'll use the following synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "reserved-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "p = 10\n",
    "\n",
    "X = np.random.normal(size=(n,p))\n",
    "y = np.dot(X, np.random.randn(p)) + .1*np.random.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-payment",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Implement a function `gradient_descent_ls(X,y,alpha,eps)` which runs gradient descent with a step size $\\alpha$, until $\\|\\nabla_b f(b)\\|_2 \\leq \\epsilon$. At each iteration $t$, compute and store the MSE $\\|Xb^{(t)} - y\\|_2^2$. Your function should return the final solution $\\hat{b}$ and a list of error from each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-immune",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "incident-necklace",
   "metadata": {},
   "source": [
    "### Part B\n",
    "Using the synethetic data above and your `gradient_descent_ls` function, run gradient descent for a variety of values $\\alpha \\in \\left(0,\\frac{1}{2\\lambda_{max}(X^\\top X)}\\right)$ (say, 5 different values). You can use a reasonable threshold for the termination condition, for example $\\epsilon = 0.1$. For each run, plot the errors as a function of the number of iterations. For which step size does the algorithm converge the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-debut",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
